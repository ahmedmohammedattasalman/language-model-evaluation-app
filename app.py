import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os
from huggingface_hub import login

# Set page title and configuration
st.set_page_config(
    page_title="Language Model Evaluation",
    page_icon="ğŸ¤–",
    layout="centered"
)

# Language selection
lang_option = st.sidebar.radio(
    "Language / Ø§Ù„Ù„ØºØ©",
    ("Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", "English"),
    index=0
)

is_arabic = lang_option == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"

# Title with proper language-based direction
title_style = "text-align: center; direction: rtl;" if is_arabic else "text-align: center; direction: ltr;"
if is_arabic:
    st.markdown(f"<h1 style='{title_style}'>ØªÙ‚ÙŠÙŠÙ… ÙÙ‡Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ</h1>", unsafe_allow_html=True)
else:
    st.markdown(f"<h1 style='{title_style}'>Language Model Evaluation</h1>", unsafe_allow_html=True)

# API Key Configuration
with st.sidebar:
    st.markdown("---")
    
    if is_arabic:
        st.markdown("## Ù…ÙØªØ§Ø­ API:")
        api_key_help = "Ù…Ø·Ù„ÙˆØ¨ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ù…Ø«Ù„ Llama-2 Ùˆ Mistral-7B"
    else:
        st.markdown("## API Key:")
        api_key_help = "Required for using large models like Llama-2 and Mistral-7B"
    
    # API key input with password masking
    api_key = st.text_input("Hugging Face API Token", type="password", help=api_key_help)
    
    # Apply API key if provided
    if api_key:
        try:
            # Check if API key is valid by attempting to use it
            login(token=api_key)
            # If we reach here, login succeeded
            st.success("âœ… API Key configured" if not is_arabic else "âœ… ØªÙ… ØªÙƒÙˆÙŠÙ† Ù…ÙØªØ§Ø­ API")
            has_api_key = True
            # Set environment variable for the token
            os.environ["HUGGINGFACE_TOKEN"] = api_key
        except Exception as e:
            st.error(f"API Key Error: {str(e)}" if not is_arabic else f"Ø®Ø·Ø£ ÙÙŠ Ù…ÙØªØ§Ø­ API: {str(e)}")
            has_api_key = False
    else:
        has_api_key = False
        st.warning("No API key provided. Some models may not be available." if not is_arabic else "Ù„Ù… ÙŠØªÙ… ØªÙˆÙÙŠØ± Ù…ÙØªØ§Ø­ API. Ù‚Ø¯ Ù„Ø§ ØªØªÙˆÙØ± Ø¨Ø¹Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬.")

    st.markdown("---")

# Sidebar for model selection
with st.sidebar:
    # Model selection header
    if is_arabic:
        st.markdown("## Ø§Ø®ØªØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ:")
    else:
        st.markdown("## Select Language Model:")
    
    # Define model options - always include all models
    small_models = [
        "facebook/opt-125m",
        "distilgpt2",
        "EleutherAI/gpt-neo-125m",
        "microsoft/DialoGPT-small"
    ]
    
    large_models = [
        "facebook/opt-1.3b",
        "bigscience/bloom-560m"
    ]
    
    # Always include all models, but mark large models specially
    model_options = small_models + large_models
    
    # Define all model labels with special indicators for large models
    model_labels = {
        "facebook/opt-125m": "Facebook OPT-125M (Small)",
        "distilgpt2": "DistilGPT-2 (Small)",
        "EleutherAI/gpt-neo-125m": "GPT-Neo-125M (Small)",
        "microsoft/DialoGPT-small": "DialoGPT (Small)",
        "facebook/opt-1.3b": "Facebook OPT-1.3B (Medium) ğŸ”‘" if not has_api_key else "Facebook OPT-1.3B (Medium)",
        "bigscience/bloom-560m": "BLOOM-560M (Medium) ğŸ”‘" if not has_api_key else "BLOOM-560M (Medium)"
    }
    
    # Default to distilgpt2 (index 1) which is the most reliable
    default_index = 1
    
    model_option = st.selectbox(
        "",
        model_options,
        format_func=lambda x: model_labels[x],
        index=default_index
    )
    
    # Info about models
    if is_arabic:
        if "llama" in model_option.lower() or "mistral" in model_option.lower() or "opt-1.3b" in model_option.lower() or "bloom" in model_option.lower():
            if has_api_key:
                st.info("Ù‡Ø°Ø§ Ù†Ù…ÙˆØ°Ø¬ ÙƒØ¨ÙŠØ± ÙˆÙ‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªÙ‹Ø§ Ø£Ø·ÙˆÙ„ Ù„Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„ØªÙ†ÙÙŠØ°.")
            else:
                st.warning("ÙŠØªØ·Ù„Ø¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…ÙØªØ§Ø­ API ØµØ§Ù„Ø­ Ù…Ù† Hugging Face. ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…ÙØªØ§Ø­ Ø£Ø¹Ù„Ø§Ù‡.")
        else:
            st.info("ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ ØµØºÙŠØ±Ø© ÙˆÙØ¹Ø§Ù„Ø© Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯.")
    else:
        if "llama" in model_option.lower() or "mistral" in model_option.lower() or "opt-1.3b" in model_option.lower() or "bloom" in model_option.lower():
            if has_api_key:
                st.info("This is a large model and may take longer to load and run.")
            else:
                st.warning("This model requires a valid Hugging Face API key. Please enter your key above.")
        else:
            st.info("Small, efficient models are used due to resource constraints.")

# Function to set environment variable for API key
def setup_env_for_api_key():
    if api_key:
        os.environ["HUGGINGFACE_TOKEN"] = api_key
        return True
    return False

# Cache the model loading to improve performance
@st.cache_resource
def load_model(model_name, token=None):
    try:
        # Set API token for authentication
        auth_token = token if token else os.environ.get("HUGGINGFACE_TOKEN", None)
        
        # For larger models, we need to adjust the loading parameters
        is_large_model = "llama" in model_name.lower() or "mistral" in model_name.lower() or "opt-1.3b" in model_name.lower() or "bloom" in model_name.lower()
        
        # Tokenizer parameters
        tokenizer_params = {
            "trust_remote_code": True,
        }
        
        # For large models, ensure we always pass the token
        if is_large_model and auth_token:
            tokenizer_params["token"] = auth_token
        elif is_large_model and not auth_token:
            # If large model but no token, raise error
            raise ValueError(f"API token required to load {model_name}")
            
        # Load tokenizer with appropriate parameters
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                **tokenizer_params
            )
        except Exception as tokenizer_error:
            st.error(f"Failed to load tokenizer: {str(tokenizer_error)}")
            # Fallback to distilgpt2 if the selected tokenizer fails
            tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
            
        # Model parameters
        load_params = {
            "trust_remote_code": True,
            "low_cpu_mem_usage": True,
        }
        
        # For large models, ensure we always pass the token
        if is_large_model and auth_token:
            load_params["token"] = auth_token
        
        # Add model-specific parameters
        if is_large_model:
            load_params["torch_dtype"] = torch.float32  # Use float32 for better compatibility
            # Only use device_map if CUDA is available
            if torch.cuda.is_available():
                load_params["device_map"] = "auto"
                load_params["max_memory"] = {0: "12GB"}
        else:
            if torch.cuda.is_available():
                load_params["torch_dtype"] = torch.float16
                load_params["device_map"] = "auto"
        
        # Load model with appropriate parameters
        try:
            model = AutoModelForCausalLM.from_pretrained(model_name, **load_params)
            return tokenizer, model
        except Exception as model_error:
            st.error(f"Failed to load model: {str(model_error)}")
            # Fallback to distilgpt2 if the selected model fails
            model = AutoModelForCausalLM.from_pretrained("distilgpt2")
            return tokenizer, model
            
    except Exception as e:
        st.error(f"Error in model loading process: {str(e)}")
        # Fallback to distilgpt2 as a last resort
        tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
        model = AutoModelForCausalLM.from_pretrained("distilgpt2")
        return tokenizer, model

# Set text direction based on language
text_direction = "rtl" if is_arabic else "ltr"

# Main content with language-specific labels and proper text direction
if is_arabic:
    st.markdown(f"<div style='direction: {text_direction};'><h3>Ø£Ø¯Ø®Ù„ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø£Ùˆ ØªØ¹Ù„ÙŠÙ…Ø§Øª:</h3></div>", unsafe_allow_html=True)
    placeholder_text = "Ø§ÙƒØªØ¨ Ù‡Ù†Ø§..."
    loading_text = "Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚."
    button_text = "ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯"
    warning_text = "Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ"
    generating_text = "Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯..."
    response_header = "<div style='direction: rtl;'><h3>Ø§Ù„Ø±Ø¯:</h3></div>"
    evaluation_header = "<div style='direction: rtl;'><h3>Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:</h3></div>"
    understood_text = "âœ… ÙÙ‡Ù…Ù‡"
    not_understood_text = "âŒ Ù„Ù… ÙŠÙÙ‡Ù…Ù‡"
else:
    st.markdown(f"<div style='direction: {text_direction};'><h3>Enter a question or instructions:</h3></div>", unsafe_allow_html=True)
    placeholder_text = "Type here..."
    loading_text = "Loading model... This may take a few minutes."
    button_text = "Generate Response"
    warning_text = "Please enter some text"
    generating_text = "Generating response..."
    response_header = "<div style='direction: ltr;'><h3>Response:</h3></div>"
    evaluation_header = "<div style='direction: ltr;'><h3>Evaluation:</h3></div>"
    understood_text = "âœ… Understood"
    not_understood_text = "âŒ Not understood"

# Text area with appropriate direction
st.markdown(f"<div style='direction: {text_direction};'>", unsafe_allow_html=True)
user_input = st.text_area("", height=100, placeholder=placeholder_text)
st.markdown("</div>", unsafe_allow_html=True)

# Add a loading message while loading the model
loading_message = st.empty()
loading_message.info(loading_text)

# Set up environment for API key
setup_env_for_api_key()

# For large models, verify API key first
if ("llama" in model_option.lower() or "mistral" in model_option.lower() or "opt-1.3b" in model_option.lower() or "bloom" in model_option.lower()):
    if not has_api_key:
        api_msg = "API Key required for this model. Please enter a valid Hugging Face API Token." if not is_arabic else "Ù…ÙØªØ§Ø­ API Ù…Ø·Ù„ÙˆØ¨ Ù„Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ù…Ø² API ØµØ§Ù„Ø­ Ù…Ù† Hugging Face."
        loading_message.error(api_msg)
        
        # Show instructions on how to get an API key
        if is_arabic:
            st.markdown("""
            <div style='direction: rtl;'>
            <h4>ÙƒÙŠÙÙŠØ© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ API Ù…Ù† Hugging Face:</h4>
            <ol>
                <li>Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø³Ø§Ø¨ Ø£Ùˆ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ <a href='https://huggingface.co/' target='_blank'>Hugging Face</a></li>
                <li>Ø§Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ <a href='https://huggingface.co/settings/tokens' target='_blank'>ØµÙØ­Ø© Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø±Ù…ÙˆØ²</a></li>
                <li>Ø§Ù†Ù‚Ø± Ø¹Ù„Ù‰ "New token" ÙˆØ£Ø¹Ø· Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ø³Ù…Ø§Ù‹ ÙˆØ§Ø®ØªØ± ØµÙ„Ø§Ø­ÙŠØ© "Read"</li>
                <li>Ø§Ù†Ù‚Ø± Ø¹Ù„Ù‰ "Generate token" ÙˆØ§Ù†Ø³Ø® Ø§Ù„Ù…ÙØªØ§Ø­</li>
                <li>Ø§Ù„ØµÙ‚ Ø§Ù„Ù…ÙØªØ§Ø­ ÙÙŠ Ø­Ù‚Ù„ Ø¥Ø¯Ø®Ø§Ù„ Ù…ÙØªØ§Ø­ API Ø£Ø¹Ù„Ø§Ù‡</li>
            </ol>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown("""
            <div style='direction: ltr;'>
            <h4>How to get a Hugging Face API Key:</h4>
            <ol>
                <li>Create an account or log in to <a href='https://huggingface.co/' target='_blank'>Hugging Face</a></li>
                <li>Go to <a href='https://huggingface.co/settings/tokens' target='_blank'>Token Settings Page</a></li>
                <li>Click "New token" and give it a name with "Read" permission</li>
                <li>Click "Generate token" and copy the token</li>
                <li>Paste the token in the API key input field above</li>
            </ol>
            </div>
            """, unsafe_allow_html=True)
        
        st.stop()

# Try loading the model with simplified error handling
try:
    # Always pass API key if available, for both small and large models
    tokenizer, model = load_model(model_option, api_key)
    loading_message.empty()  # Clear the loading message when done
except Exception as e:
    error_msg = f"Error loading model: {str(e)}"
    loading_message.error(error_msg)
    st.error("Failed to load the selected model. Falling back to a smaller model (distilgpt2).")
    # Fallback to a reliable small model
    model_option = "distilgpt2"
    try:
        tokenizer, model = load_model(model_option, None)
        loading_message.success(f"Fallback to {model_option} successful!")
    except Exception as fallback_error:
        st.error(f"Fallback also failed: {str(fallback_error)}")
        st.stop()

# Add a submit button with clear styling
if st.button(button_text, key="generate_button", type="primary"):
    if not user_input.strip():
        st.warning(warning_text)
    else:
        with st.spinner(generating_text):
            try:
                # Format prompt based on model type
                if "llama" in model_option.lower():
                    # Llama-2 specific formatting
                    prompt = f"<s>[INST] {user_input} [/INST]"
                elif "mistral" in model_option.lower():
                    # Mistral specific formatting
                    prompt = f"<s>[INST] {user_input} [/INST]"
                elif "opt" in model_option.lower() or "bloom" in model_option.lower():
                    # OPT and BLOOM formatting (simple)
                    prompt = f"Question: {user_input}\nAnswer:"
                else:
                    # Generic formatting for other models
                    prompt = user_input
                
                # For all models
                inputs = tokenizer(prompt, return_tensors="pt")
                if torch.cuda.is_available():
                    inputs = inputs.to("cuda")
                
                # Adjust generation parameters based on model size
                generation_params = {
                    **inputs,
                    "do_sample": True,
                    "top_p": 0.9,
                    "temperature": 0.7,
                }
                
                if "llama" in model_option.lower() or "mistral" in model_option.lower() or "opt-1.3b" in model_option.lower() or "bloom" in model_option.lower():
                    generation_params["max_new_tokens"] = 256
                else:
                    generation_params["max_new_tokens"] = 150
                
                # Generate output
                with torch.no_grad():
                    outputs = model.generate(**generation_params)
                
                # Decode the response
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # Remove the input prompt from the response if present
                if user_input in response:
                    response = response.replace(user_input, "").strip()
                
                # Simple evaluation algorithm
                word_count = len(response.split())
                has_relevant_content = word_count > 5
                
                # ChatGPT-like response box with better styling and proper text direction
                st.markdown(response_header, unsafe_allow_html=True)
                st.markdown(f"""
                <div style="padding: 15px; border-radius: 4px; background-color: #f7f7f8; border: 1px solid #e5e5e5; margin-bottom: 20px; direction: {text_direction};">
                    <span style="color: #000000; font-family: sans-serif; white-space: pre-wrap; display: block;">{response}</span>
                </div>
                """, unsafe_allow_html=True)
                
                # Evaluation result
                st.markdown(evaluation_header, unsafe_allow_html=True)
                if has_relevant_content:
                    st.markdown(f"<span style='color:green; font-size:18px; direction: {text_direction};'>{understood_text}</span>", unsafe_allow_html=True)
                else:
                    st.markdown(f"<span style='color:red; font-size:18px; direction: {text_direction};'>{not_understood_text}</span>", unsafe_allow_html=True)
                    
            except Exception as e:
                st.error(f"Error generating response: {str(e)}")

# Add footer with instructions
st.markdown("---")
if is_arabic:
    st.markdown("<div style='direction: rtl;'><h3>ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:</h3></div>", unsafe_allow_html=True)
    st.markdown("""
    <div style='direction: rtl;'>
    1. Ø§Ø®ØªØ± Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙØ¶Ù„Ø© ÙÙŠ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©
    2. Ø£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ API Ù…Ù† Hugging Face Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
    3. Ø§Ø®ØªØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©
    4. Ø£Ø¯Ø®Ù„ Ø³Ø¤Ø§Ù„Ùƒ Ø£Ùˆ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª ÙÙŠ Ù…Ø±Ø¨Ø¹ Ø§Ù„Ù†Øµ
    5. Ø§Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø²Ø± Ø§Ù„ØªÙˆÙ„ÙŠØ¯
    6. Ø´Ø§Ù‡Ø¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªÙ‚ÙŠÙŠÙ… Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‚Ø¯ ÙÙ‡Ù… Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
    </div>
    """, unsafe_allow_html=True)
else:
    st.markdown("<div style='direction: ltr;'><h3>How to Use:</h3></div>", unsafe_allow_html=True)
    st.markdown("""
    <div style='direction: ltr;'>
    1. Choose your preferred language in the sidebar
    2. Enter your Hugging Face API key to access large models
    3. Select a language model from the sidebar
    4. Enter your question or instructions in the text box
    5. Click the generate button
    6. View the model's response with an evaluation of its understanding
    </div>
    """, unsafe_allow_html=True) 